Absolutely. Hereâ€™s a massive high-context prompt designed for Cursorâ€™s GitHub Copilot Agent or any advanced AI assistant (e.g., GPT-4o with code context). This prompt is structured to help it act like a co-developer for your RL market-making simulator project.

â¸»

ğŸ§  Full Prompt for Cursor GitHub Copilot Agent

You are assisting me in building an advanced, production-grade **Reinforcement Learning Market Making Simulator** for high-frequency trading, using **real Nasdaq LOB data** via API.

## ğŸ¯ Objective
Build a deep RL agent (using TD3 or DDPG) that learns to optimally quote bid/ask prices in a simulated limit order book (LOB) environment. The simulator uses real Nasdaq order flow data, and the goal is to maximize long-term PnL while managing inventory and execution risk.

---

## ğŸ“š Data Pipeline

1. **Source**: Nasdaq API (TotalView-like depth-of-book feed).
2. **Ingestion**:
   - Pull microsecond-level quote and trade data (L2).
   - Parse into time-synced LOB snapshots (top 10 levels).
   - Match orders and compute microprice, imbalance, order flow stats.
3. **Format**:
   - Snapshot format: depth, volume, price, timestamp.
   - Trade logs: price, size, side, latency.

---

## âš™ï¸ LOB Simulator Requirements

- FIFO matching engine.
- Synthetic fill logic if real data is incomplete.
- Quote aging and time-to-live (TTL).
- Agentâ€™s quotes should be visible to the simulated book.
- Optionally, simulate latency or queue priority (quote age).
- Allow both synthetic order flow simulation and replay from real data.

---

## ğŸ§  Agent Environment

### State Vector (features):
- Top 10 bid/ask levels: price & volume
- LOB imbalance (volume-weighted)
- Order flow imbalance (recent trade history)
- Microprice
- Realized volatility over last N steps
- Inventory
- Quote age
- Time since last fill

### Action Space:
- **Continuous (TD3/DDPG)**:
  - `a1`: % offset from mid-price for bid
  - `a2`: % offset from mid-price for ask

### Reward Function:
```python
reward = delta_pnl - lambda * abs(inventory)

Where:
	â€¢	delta_pnl = mark-to-market PnL since last step
	â€¢	inventory = signed inventory (positive: long, negative: short)
	â€¢	lambda = inventory penalty coefficient

â¸»

ğŸ§ª RL Training Setup
	â€¢	Algorithm: TD3 (preferred) or DDPG
	â€¢	Framework: PyTorch or TensorFlow
	â€¢	Replay buffer: >500,000 transitions
	â€¢	Use Prioritized Experience Replay (optional)
	â€¢	Normalize inputs per episode or daily session
	â€¢	Implement target policy smoothing & delayed updates (TD3 tricks)

â¸»

ğŸ“Š Evaluation Metrics
	â€¢	Daily PnL, Sharpe Ratio
	â€¢	Fill rate
	â€¢	Spread width vs. execution quality
	â€¢	Inventory RMS
	â€¢	Time in/out of market
	â€¢	Slippage and adverse selection

â¸»

ğŸ’¡ Stretch Goals
	â€¢	Multi-agent self-play (quote agent vs. flow agent)
	â€¢	Transformer encoder for temporal state
	â€¢	Real-time inference engine
	â€¢	Integration with WebSocket feed (live paper trading)
	â€¢	C++ LOB engine for high-speed simulation

â¸»

ğŸ“ File Structure (Target)

market_maker_rl/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Raw Nasdaq API dumps
â”‚   â”œâ”€â”€ processed/        # Cleaned LOB snapshots
â”‚
â”œâ”€â”€ simulator/
â”‚   â”œâ”€â”€ lob_simulator.py  # Matching engine + order logic
â”‚   â”œâ”€â”€ market_env.py     # Gym-like RL interface
â”‚
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ td3_agent.py
â”‚   â”œâ”€â”€ networks.py
â”‚   â”œâ”€â”€ replay_buffer.py
â”‚
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ train_td3.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ td3_config.yaml
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â”œâ”€â”€ data_utils.py
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ eda.ipynb         # Market microstructure EDA
â”‚   â”œâ”€â”€ strategy_backtest.ipynb
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_lob.py
â”‚   â”œâ”€â”€ test_agent.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md


â¸»

ğŸ”’ Development Rules & Best Practices
	1.	No assumptions about data â€” check nulls, missing trades, quote updates.
	2.	Use modular components: keep LOB logic independent from agent logic.
	3.	Test every component: quote matching, fill logic, inventory updates.
	4.	Keep latency awareness even in simulation: quote timing matters.
	5.	Avoid magic constants: define tunables in YAML config.
	6.	Comment every function and class clearly.
	7.	Use wandb or tensorboard for experiment tracking.
	8.	RL episodes should simulate full trading sessions (e.g., 9:30â€“16:00 EST).
	9.	When debugging, use offline logs and replay visualizations of order book events.
	10.	Avoid using black-box rewards â€” keep reward shaping transparent.

â¸»

You are now acting as a co-developer. Begin by scaffolding the simulator, and then help implement RL components one at a time.

---

## âœ… What This Will Trigger in Cursor

With this context:
- Copilot Agent can auto-complete file structures.
- You can type prompts like â€œcreate the matching engineâ€ or â€œimplement TD3 replay buffer,â€ and it will do so with full understanding.
- Cursor can help you test/debug code as itâ€™s being built, simulate actions, and even visualize metrics inline.

---

Would you like me to generate the actual `lob_simulator.py` file scaffold as the first task?